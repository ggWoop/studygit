{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816b6255-672c-4ad5-880c-98c70728e5a4",
   "metadata": {},
   "source": [
    "## 영문 텍스트 토큰화 하기\n",
    "\n",
    "데이터 셋 출처: https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb\n",
    "\n",
    "연습문제는 영문 텍스트로 진행할 예정입니다. 영문도 마찬가지로 토큰화를 적용해주어야 하는데, 한국어와 몇가지 다른 포인트들이 있습니다. 영어는 먼저 대소문자를 신경써주어야 하며, ing나 ed와 같이 시제를 나타내는 조사를 분리해주어야 합니다. 그리고 경우에 따라서 불용어를 처리해주어야 합니다.\n",
    "\n",
    "영문 토큰화에는 nltk 라이브러리가 대표적입니다. nltk를 사용하여 영문 텍스트를 전처리해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da42393-4038-47fc-a916-354ded5b3708",
   "metadata": {},
   "source": [
    "### 영어 문장 토큰화\n",
    "\n",
    "영어 문장은 소문자로 변경한 뒤, nltk 라이브러리에 내장된 WordPunctTokenizer를 이용하여 토큰화 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07e0ecd-d237-4fbd-ad68-5a5e93aace92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732583ba-92a6-4e00-b9ee-589fe39a72c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WOODLAC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\WOODLAC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WOODLAC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')\n",
    "stop_words_list = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    filtered_tokens = [x for x in stems if x not in stop_words_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30835a5e-592c-47b4-9f2f-5e447c522793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            title        genre  \\\n",
      "0                    Oscar et la dame rose (2009)        drama   \n",
      "1                                    Cupid (1997)     thriller   \n",
      "2                Young, Wild and Wonderful (1980)        adult   \n",
      "3                           The Secret Sin (1915)        drama   \n",
      "4                          The Unrecovered (2007)        drama   \n",
      "...                                           ...          ...   \n",
      "54209                             \"Bonino\" (1953)       comedy   \n",
      "54210                 Dead Girls Don't Cry (????)       horror   \n",
      "54211   Ronald Goedemondt: Ze bestaan echt (2008)  documentary   \n",
      "54212                    Make Your Own Bed (1944)       comedy   \n",
      "54213  Nature's Fury: Storm of the Century (2006)      history   \n",
      "\n",
      "                                                 content  \\\n",
      "0      Listening in to a conversation between his doc...   \n",
      "1      A brother and sister with a past incestuous re...   \n",
      "2      As the bus empties the students for their fiel...   \n",
      "3      To help their unemployed father make ends meet...   \n",
      "4      The film's title refers not only to the un-rec...   \n",
      "...                                                  ...   \n",
      "54209  This short-lived NBC live sitcom centered on B...   \n",
      "54210  The NEXT Generation of EXPLOITATION. The siste...   \n",
      "54211  Ze bestaan echt, is a stand-up comedy about gr...   \n",
      "54212  Walter and Vivian live in the country and have...   \n",
      "54213  On Labor Day Weekend, 1935, the most intense h...   \n",
      "\n",
      "                                       processed_content  \n",
      "0      [listening, conversation, doctor, parent, ,, 1...  \n",
      "1      [brother, sister, past, incestuous, relationsh...  \n",
      "2      [bus, empty, student, field, trip, museum, nat...  \n",
      "3      [help, unemployed, father, make, end, meet, ,,...  \n",
      "4      [film, 's, title, refers, un-recovered, body, ...  \n",
      "...                                                  ...  \n",
      "54209  [short-lived, nbc, live, sitcom, centered, bon...  \n",
      "54210  [next, generation, exploitation, ., sister, ka...  \n",
      "54211  [ze, bestaan, echt, ,, stand-up, comedy, growi...  \n",
      "54212  [walter, vivian, live, country, difficult, tim...  \n",
      "54213  [labor, day, weekend, ,, 1935, ,, intense, hur...  \n",
      "\n",
      "[54214 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df['processed_content'] = df['content'].apply(preprocess)\n",
    "\n",
    "# Print the dataframe to see the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d018eeb-949e-4965-8b99-1ded937725c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening in to a conversation between his doctor and parents, 10-year-old Oscar learns what nobody has the courage to tell him. He only has a few weeks to live. Furious, he refuses to speak to anyone except straight-talking Rose, the lady in pink he meets on the hospital stairs. As Christmas approaches, Rose uses her fantastical experiences as a professional wrestler, her imagination, wit and charm to allow Oscar to live life and love to the full, in the company of his friends Pop Corn, Einstein, Bacon and childhood sweetheart Peggy Blue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca021e81-0edb-49e5-a5b3-0c042882c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ffc6a30-c07b-458a-a559-b5fb4d74b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listening', 'in', 'to', 'a', 'conversation', 'between', 'his', 'doctor', 'and', 'parents', ',', '10', '-', 'year', '-', 'old', 'oscar', 'learns', 'what', 'nobody', 'has', 'the', 'courage', 'to', 'tell', 'him', '.', 'he', 'only', 'has', 'a', 'few', 'weeks', 'to', 'live', '.', 'furious', ',', 'he', 'refuses', 'to', 'speak', 'to', 'anyone', 'except', 'straight', '-', 'talking', 'rose', ',', 'the', 'lady', 'in', 'pink', 'he', 'meets', 'on', 'the', 'hospital', 'stairs', '.', 'as', 'christmas', 'approaches', ',', 'rose', 'uses', 'her', 'fantastical', 'experiences', 'as', 'a', 'professional', 'wrestler', ',', 'her', 'imagination', ',', 'wit', 'and', 'charm', 'to', 'allow', 'oscar', 'to', 'live', 'life', 'and', 'love', 'to', 'the', 'full', ',', 'in', 'the', 'company', 'of', 'his', 'friends', 'pop', 'corn', ',', 'einstein', ',', 'bacon', 'and', 'childhood', 'sweetheart', 'peggy', 'blue', '.']\n"
     ]
    }
   ],
   "source": [
    "sample = df.iloc[0][\"content\"].lower()\n",
    "print(tokenizer.tokenize(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc5474-937a-4fbb-9adb-01e408603085",
   "metadata": {},
   "source": [
    "### 어간 추출\n",
    "\n",
    "한국어에서 조사가 붙듯이 영어에서도 ing, ed 등 시제나 현재 진행 등을 나타내는 문법적인 요소들이 붙습니다. 이를 분리해내려면 stemming이라는 기법을 사용할 수 있습니다. stemming에 대한 자세한 내용은 생략하고, 가장 대표적인 stemming 알고리즘인 PorterStem을 이용하여 어간을 추출해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5ef9432-2acc-4891-ae91-ca2be3ab5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2e02a43-0e37-40c6-a7f1-eceabfe07cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'in', 'to', 'a', 'convers', 'between', 'hi', 'doctor', 'and', 'parent', ',', '10', '-', 'year', '-', 'old', 'oscar', 'learn', 'what', 'nobodi', 'ha', 'the', 'courag', 'to', 'tell', 'him', '.', 'he', 'onli', 'ha', 'a', 'few', 'week', 'to', 'live', '.', 'furiou', ',', 'he', 'refus', 'to', 'speak', 'to', 'anyon', 'except', 'straight', '-', 'talk', 'rose', ',', 'the', 'ladi', 'in', 'pink', 'he', 'meet', 'on', 'the', 'hospit', 'stair', '.', 'as', 'christma', 'approach', ',', 'rose', 'use', 'her', 'fantast', 'experi', 'as', 'a', 'profession', 'wrestler', ',', 'her', 'imagin', ',', 'wit', 'and', 'charm', 'to', 'allow', 'oscar', 'to', 'live', 'life', 'and', 'love', 'to', 'the', 'full', ',', 'in', 'the', 'compani', 'of', 'hi', 'friend', 'pop', 'corn', ',', 'einstein', ',', 'bacon', 'and', 'childhood', 'sweetheart', 'peggi', 'blue', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample)\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7589071-78cb-4c49-9802-6cfe428ebda9",
   "metadata": {},
   "source": [
    "### 불용어 처리\n",
    "\n",
    "영어에는 자주 a, and, as와 같이 자주 사용되는 어휘들이 있습니다. 이들은 자주 사용되지만 큰 의미가 없는 토큰으로 간주하여 삭제 처리하겠습니다. 이를 불용어(stop word) 처리라고 부릅니다. (물론 선택에 따라서 삭제하지 않을 수도 있으며, 현대의 딥러닝 모델들은 굳이 삭제하지 않습니다.) \n",
    "\n",
    "마찬가지로 nltk에 내장되어 있는 stopword를 이용해 불용어 처리를 진행하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "503d4d6b-e146-4b44-8e62-1975e7d03429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "114cc66a-13c1-4663-8d62-d38370069951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a59330b2-a6e6-4ea2-9a7d-920f81c5ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    filtered_tokens = [x for x in stems if x not in stop_words_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18c9c3e2-fae4-4223-881f-814d5b605567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 54214/54214 [00:43<00:00, 1243.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"content_tokens\"] = df[\"content\"].progress_apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82cdb39c-1864-4a20-ada9-6ac7292cef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A brother and sister with a past incestuous relationship have a current murderous relationship. He murders the women who reject him and she murders the women who get too close to him.\n",
      "\n",
      "['brother', 'sister', 'past', 'incestu', 'relationship', 'current', 'murder', 'relationship', '.', 'murder', 'women', 'reject', 'murder', 'women', 'get', 'close', '.']\n"
     ]
    }
   ],
   "source": [
    "target_idx = 1\n",
    "print(df.iloc[target_idx][\"content\"])\n",
    "print(df.iloc[target_idx][\"content_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb1a65-b9b4-4794-9afc-79750dfd2733",
   "metadata": {},
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e135fdd5-09b2-4905-8cea-fd6ad263258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/movies_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0a756-9efc-477f-abed-6805c4e9fd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
